{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read details of the environment & model from a text file\n",
    "Read: \n",
    "\n",
    "- Width, Height\n",
    "- Noise: probability of not going in the intended direction (then divided by two for two unintentional neighbor directions)\n",
    "- Immediate rewards of non-goal states\n",
    "- Terminal (goal) states: their locations and rewards\n",
    "- Internal Walls: locations\n",
    "\n",
    "Create the transition matrix (T) and the Rewards matrix according to above details.\n",
    "\n",
    "\"grid1.txt\" corresponds to the example in AIMA and the lecture slides. You can double-check various results there.\n",
    "\n",
    "Python style comments in grid1.txt are just comments, can be ignored/deleted.\n",
    "\n",
    "Task1 - We ask you to create two more grids (files) of moderate size and with at least one internal wall. And perform the analysis described below on these three grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "file1 = open('grid1.txt', 'r') \n",
    "(W, H) = [t(s) for t,s in zip((int,int),file1.readline().split())]\n",
    "noise = [t(s) for t,s in zip((float,str),file1.readline().split())][0]\n",
    "immediate_rewards = [t(s) for t,s in zip((float,str),file1.readline().split())][0]\n",
    "N = W*H #total number of states\n",
    "NA = 4 #number of actions\n",
    "T = np.zeros((N,N,NA)) #state transition probabilities\n",
    "Directions = np.array(['L','R','U','D'])\n",
    "Rewards = np.zeros((N,1)) + immediate_rewards #immediate rewards\n",
    "Terminals = np.zeros((N,1)) #terminal states\n",
    "Walls = np.zeros((N,1)) #(internal) wall states\n",
    "prob1 = 1-noise\n",
    "prob2 = noise/2\n",
    "while True: \n",
    "    line = file1.readline() \n",
    "    if not line: \n",
    "        break\n",
    "    if line.find('T') != -1:\n",
    "        (_,x, y, r) = [t(s) for t,s in zip((str,int,int,float),line.split())]\n",
    "        Terminals[y*W+x] = 1\n",
    "        Rewards[y*W+x] = r\n",
    "    if line.find('W') != -1:\n",
    "        (_,x, y) = [t(s) for t,s in zip((str,int,int),line.split())]\n",
    "        Walls[y*W+x] = 1     \n",
    "file1.close()\n",
    "for i in range(W):\n",
    "    for j in range(H):\n",
    "        if i+1<W and Walls[j*W+i+1]==0: #not wall on the right\n",
    "            T[j*W+i,j*W+i+1,1] += prob1 \n",
    "            T[j*W+i,j*W+i+1,2] += prob2\n",
    "            T[j*W+i,j*W+i+1,3] += prob2\n",
    "        else:\n",
    "            T[j*W+i,j*W+i,1] += prob1 \n",
    "            T[j*W+i,j*W+i,2] += prob2\n",
    "            T[j*W+i,j*W+i,3] += prob2\n",
    "        if i-1>=0 and Walls[j*W+i-1]==0: #not wall on the left\n",
    "            T[j*W+i,j*W+i-1,0] += prob1 \n",
    "            T[j*W+i,j*W+i-1,2] += prob2\n",
    "            T[j*W+i,j*W+i-1,3] += prob2\n",
    "        else:\n",
    "            T[j*W+i,j*W+i,0] += prob1 \n",
    "            T[j*W+i,j*W+i,2] += prob2\n",
    "            T[j*W+i,j*W+i,3] += prob2\n",
    "        if j+1<H and Walls[(j+1)*W+i]==0: #not wall below\n",
    "            T[j*W+i,(j+1)*W+i,3] += prob1 \n",
    "            T[j*W+i,(j+1)*W+i,0] += prob2\n",
    "            T[j*W+i,(j+1)*W+i,1] += prob2\n",
    "        else:\n",
    "            T[j*W+i,j*W+i,3] += prob1 \n",
    "            T[j*W+i,j*W+i,0] += prob2\n",
    "            T[j*W+i,j*W+i,1] += prob2       \n",
    "        if j-1>=0 and Walls[(j-1)*W+i]==0: #not wall above\n",
    "            T[j*W+i,(j-1)*W+i,2] += prob1 \n",
    "            T[j*W+i,(j-1)*W+i,0] += prob2\n",
    "            T[j*W+i,(j-1)*W+i,1] += prob2\n",
    "        else:\n",
    "            T[j*W+i,j*W+i,2] += prob1 \n",
    "            T[j*W+i,j*W+i,0] += prob2\n",
    "            T[j*W+i,j*W+i,1] += prob2    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Implement functions\n",
    "\n",
    "### Value and Policy iteration\n",
    "Value iteration is provided, implement policy iteration\n",
    "\n",
    "### TD-Learning and Q-Learning\n",
    "Q-Learning is implemented, implement TD-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(gamma, T, Rewards, Terminals, Walls):   \n",
    "    N, NA = T.shape[0], T.shape[2]\n",
    "    V = np.zeros((N,1))\n",
    "    Policy = np.zeros((N,1))\n",
    "    while True:\n",
    "        V_old = np.copy(V)\n",
    "        for s in range(N):\n",
    "            if Walls[s]==1: continue \n",
    "            if Terminals[s]==1:\n",
    "                V[s] = Rewards[s]\n",
    "                continue\n",
    "            Q = np.zeros((NA,1))\n",
    "            for a in range(NA):\n",
    "                Q[a] = Rewards[s] + gamma*np.dot(T[s,:,a],V)\n",
    "            V[s] = np.max(Q)\n",
    "            Policy[s] = np.argmax(Q)\n",
    "        if np.sum(np.abs(V-V_old))<1e-10:\n",
    "            break\n",
    "    return V, Policy\n",
    "\n",
    "def policy_iteration(gamma, T, Rewards, Terminals, Walls): \n",
    "    N, NA = T.shape[0], T.shape[2]\n",
    "    V = np.zeros((N,1))\n",
    "    Policy = np.zeros((N,1))    \n",
    "    # Implement PI\n",
    "    return V, Policy\n",
    "\n",
    "def TD_learning(N_episodes, alpha, gamma, T, Rewards, Terminals, Walls): \n",
    "    N, NA = T.shape[0], T.shape[2]\n",
    "    V = np.zeros((N,1))\n",
    "    Policy = np.zeros((N,1)) \n",
    "    # Implement TD\n",
    "    return V, Policy\n",
    "\n",
    "def Q_learning(N_episodes, epsilon, alpha, gamma, T, Rewards, Terminals, Walls): \n",
    "    N, NA = T.shape[0], T.shape[2]\n",
    "    Q = np.zeros((N,NA))\n",
    "    for e in range(N_episodes):\n",
    "        s = int(np.floor(np.random.uniform(0,N-1)))\n",
    "        while Terminals[s]==1 or Walls[s]==1:\n",
    "            s = int(np.floor(np.random.uniform(0,N-1)))\n",
    "        while Terminals[s]==0:\n",
    "            u = np.random.uniform(0,1)\n",
    "            if u<epsilon:\n",
    "                a = int(np.floor(np.random.uniform(0,NA)))\n",
    "            else:\n",
    "                a = np.argmax(Q[s,:])\n",
    "            u = np.random.uniform(0,1)\n",
    "            s1 = np.argmax(u<np.cumsum(T[s,:,a]))\n",
    "            Q[s,a] += alpha*(Rewards[s1] + gamma*np.max(Q[s1,:]) - Q[s,a])\n",
    "            s = s1\n",
    "        epsilon = epsilon*0.9999 # Modify here for other annealing regimes\n",
    "        alpha = alpha*0.9999 # Modify here for other annealing regimes\n",
    "    V = np.max(Q,axis=1)\n",
    "    V = V[:,None]\n",
    "    V[Terminals==1] = Rewards[Terminals==1] \n",
    "    Policy = np.array(np.argmax(Q,axis=1)).reshape((N,1))\n",
    "    return V, Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Experiments\n",
    "\n",
    "For the three grids you have, perform the following analysis and include them in your report,.\n",
    "\n",
    "- Compare value and policy iterations in terms of convergence time. Relate it to computational complexity, current implementation and number of iterations needed.\n",
    "\n",
    "- How are optimal policies change with immediate reward values? Show some examples (similar to Figure 17.2b in AIMA)\n",
    "\n",
    "- Compare TD-Learning and Q-Learning results with each other. Also with value and policy iterations. Remember that value and policy iteration solve Markov Decision Processes where we know the model (T and Rewards). TD-Learning and Q-Learning are (passive and active, respectively) Reinforcement Learning methods that don't have the model but use data from simulations. Data are simulated from the model we know, but the model is not used n TD or Q-Learning.\n",
    "\n",
    "- What are the effects of epsilon and alpha values and how they are modified?\n",
    "\n",
    "- What is the effect of number of episodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.81155822  0.86780822  0.91780822  1.        ]\n",
      " [ 0.76155822  0.          0.66027397 -1.        ]\n",
      " [ 0.70530822  0.65530822  0.61141553  0.38792491]]\n",
      "[['R' 'R' 'R' 'G']\n",
      " ['U' 'W' 'U' 'G']\n",
      " ['U' 'L' 'L' 'L']]\n",
      "[[ 0.86613487  0.93543322  0.97931567  1.        ]\n",
      " [ 0.80673034  0.          0.92747207 -1.        ]\n",
      " [ 0.7516711   0.70131802  0.66082128  0.57232503]]\n",
      "[['R' 'R' 'R' 'G']\n",
      " ['U' 'W' 'U' 'G']\n",
      " ['U' 'L' 'L' 'L']]\n"
     ]
    }
   ],
   "source": [
    "gamma = 1\n",
    "alpha = 0.9\n",
    "epsilon = 0.5\n",
    "N_episodes = 20000\n",
    "\n",
    "#Value iteration\n",
    "V_VI, Policy_VI = value_iteration(gamma, T, Rewards, Terminals, Walls)\n",
    "#Policy iteration\n",
    "V_PI, Policy_PI = policy_iteration(gamma, T, Rewards, Terminals, Walls)\n",
    "#TD-Learning\n",
    "V_TD, Policy_TD = TD_learning(N_episodes, alpha, gamma, T, Rewards, Terminals, Walls)\n",
    "#Q-Learning\n",
    "V_Q, Policy_Q = Q_learning(N_episodes, epsilon, alpha, gamma, T, Rewards, Terminals, Walls)\n",
    "\n",
    "print(V_VI.reshape((H,W)))\n",
    "maze = Directions[Policy_VI.astype(int)]\n",
    "maze[Walls==1] = 'W'\n",
    "maze[Terminals==1] = 'G'\n",
    "print(maze.reshape((H,W)))\n",
    "#\n",
    "print(V_Q.reshape((H,W)))\n",
    "maze_Q = Directions[Policy_Q.astype(int)]\n",
    "maze_Q[Walls==1] = 'W'\n",
    "maze_Q[Terminals==1] = 'G'\n",
    "print(maze_Q.reshape((H,W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (extra credit) - What if diagonal moves were possible?\n",
    "\n",
    "Repeat the experiments in one of these settings. You don't have to stick to the code provided here. But, you can only use Numpy (and matplotlib).\n",
    "\n",
    "You should decide on how to distribute the noise associated with actions across (next) states. For example, in the scenario above we have probability of going Up with the Up action is 1-noise. The probability of going Left with the Up action is noise/2. The same for going Right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
